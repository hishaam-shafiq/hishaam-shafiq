import numpy as np
import matplotlib.pyplot as plt

from networks import FeedforwardNetwork, SimplePerceptron

class MultilayerNetwork:
    """
    base class of multi-layer network, each layer is a FeedforwardNetwork.
    """
    
    def __init__(self, N_list, input_layer=None, connect_list=None, activate='threshold'):
        """
        declare internal variables.
        inputs:
        N_list: list of int, numbers of units in every layer.
        input_layer: 1-d array, input units; can also be pointer to array.
        connect_list: list of 2-d arrays, connection matrices between every consecutive layers.
        activate: 'threshold'|'linear'|other, activation function, can be user-supplied.
        """
        self.N_list = np.asarray(N_list)
        self.L = len(N_list) - 1    # number of layers (excluding input layer)
        if input_layer is None:
            self.input = np.zeros(N_list[0])    # first layer is input
        else:
            self.input = input_layer
        layer = FeedforwardNetwork(N_list[0], N_list[1], activate=activate)    # first layer
        layer.set_network(input=self.input)    # input = network input
        self.layers = [layer]    # list of layers (excluding input layer)
        for l in range(1,self.L):
            layer = FeedforwardNetwork(N_list[l], N_list[l+1], activate=activate)    # new layer
            layer.set_network(input=self.layers[l-1].output)    # input = previous layer's output
            self.layers.append(layer)    # add layer to network
        for l in range(self.L):
            if connect_list is None:
                rand = np.random.randn(N_list[l+1], N_list[l]) * 0.1
                self.layers[l].set_network(connect=rand)    # initialize connection to small random numbers
            else:
                self.layers[l].set_network(connect=connect_list[l])    # set connection of each layer
        self.output = self.layers[-1].output    # output of final layer
    
    def run(self, input=None):
        """
        run network to generate output, using input if given.
        inputs:
        input: 1-d array, state of input units.
        outputs:
        output: 1-d array, state of output units (of the last layer).
        """
        if input is not None:
            self.layers[0].input[:] = input
        for l in range(self.L):
            self.layers[l].run()    # run every layer
        return self.output
        
class MultilayerPerceptron(MultilayerNetwork):
    """
    multi-layer feedforward network with perceptron learning rule.
    """
    
    def train(self, input, output, learning_rate=None):
        """
        train network using back-propagation.
        inputs:
        input: 1-d array, given input values from training data.
        output: 1-d array, target output values from training data.
        learning_rate: float, learning rate, should be a small number.
        """
        if learning_rate is None:
            learning_rate = 1/max(self.N_list)
        out = self.run(input)    # generate output with current connections
        err = output - out    # difference between target output and current output
        for l in reversed(range(self.L)):    # start from last layer
            inp = self.layers[l].input    # input of l-th layer
            upd = learning_rate * err[:,np.newaxis] * inp    # learning rule
            err = np.dot(err, self.layers[l].connect)    # back-propagate error to previous layer
            self.layers[l].connect += upd    # update connection matrix
       
input_array = np.array([[0.7, 1.],
                        [1., 0.7],
                        [0.7, -0.5],
                        [-0.3, -1.],
                        [0.4, -0.9],
                        [-0.1, -0.4],
                        [0.6, -0.1],
                        [0.8, 0.1]])    # input patterns
target_array = np.array([[1],
                         [1],
                         [1],
                         [1],
                         [-1],
                         [-1],
                         [-1],
                         [-1]])    # target patterns
M = len(input_array)    # number of patterns

plt.figure(figsize=(4,4))
plt.scatter(input_array[:,0], input_array[:,1], c=target_array[:,0], ec='k', cmap='Greys')
plt.axvline(0, color='k', lw=1)
plt.axhline(0, color='k', lw=1)
plt.xlim(-1.2, 1.2)
plt.ylim(-1.2, 1.2)
plt.xlabel(r'$S_0$')
plt.ylabel(r'$S_1$', rotation=0)
plt.title('targets')
plt.show()

Ni = input_array.shape[1]    # size of input layer
No = target_array.shape[1]    # size of output layer
net = SimplePerceptron(Ni, No)    # create simple perceptron network

T = 200    # training time
seq = np.random.randint(M, size=T)    # random sequences of input patterns

for s in seq:
    net.train(input_array[s], target_array[s])    # train network by presenting input-output pairs

Ni = input_array.shape[1]    # size of input layer
No = target_array.shape[1]    # size of output layer
net = SimplePerceptron(Ni, No)    # create simple perceptron network

T = 200    # training time
seq = np.random.randint(M, size=T)    # random sequences of input patterns

for s in seq:
    net.train(input_array[s], target_array[s])    # train network by presenting input-output pairs
 
Ni = input_array.shape[1]    # size of input layer
Nh = 10*Ni    # size of hidden layer
No = target_array.shape[1]    # size of output layer
net = MultilayerPerceptron([Ni, Nh, No])    # create network

T = 1000    # training time
seq = np.random.randint(M, size=T)    # random sequence of indices for input patterns

for s in seq:
    net.train(input_array[s], target_array[s], learning_rate=0.01)    # train network by presenting input-output pairs
  
output_array = []
for inp in input_array:
    out = net.run(inp).copy()    # make a copy so that it will not be overwritten
    output_array.append(out)
output_array = np.asarray(output_array)

plt.figure(figsize=(4,4))
plt.scatter(input_array[:,0], input_array[:,1], c=output_array[:,0], ec='k', cmap='Greys')
plt.axvline(0, color='k', lw=1)
plt.axhline(0, color='k', lw=1)
plt.xlim(-1.2, 1.2)
plt.ylim(-1.2, 1.2)
plt.xlabel(r'$S_0$')
plt.ylabel(r'$S_1$', rotation=0)
plt.title('multilayer perceptron output')
plt.show()
